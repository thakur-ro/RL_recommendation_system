{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import time\n",
    "import tqdm\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/sample_df.csv')\n",
    "sequence_df = pd.read_csv('data/preprocessed_df.csv')\n",
    "embeddings_df = pd.read_csv('data/embeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_convert(sequence_data):\n",
    "    \"\"\"Seperate ratings and items\"\"\"\n",
    "    for col in ['state', 'next_state', 'action_reward']:\n",
    "        sequence_data[col] = [np.array([[k for k in ee.split('&')] for ee in e.split('|')]) for e in sequence_data[col]]\n",
    "    for col in ['state', 'next_state']:\n",
    "        sequence_data[col] = [np.array([e[0] for e in l]) for l in sequence_data[col]]\n",
    "\n",
    "    sequence_data['action'] = [[e[0] for e in l] for l in sequence_data['action_reward']]\n",
    "    sequence_data['reward'] = [tuple(e[1] for e in l) for l in sequence_data['action_reward']]\n",
    "    sequence_data.drop(columns=['action_reward'], inplace=True)\n",
    "    return sequence_data   \n",
    "def read_embeddings(embeddings_file):\n",
    "    '''Load embeddings (a vector for each item).''' \n",
    "    return np.array([[np.float64(k) for k in e.split('|')]\n",
    "                   for e in embeddings_file['embedding']])\n",
    "def create_item_mappings(embeddings_df):\n",
    "    \"\"\"since items are strings, map them with integer index\"\"\"\n",
    "    item_mappings_dict = {}\n",
    "    for _,row in embeddings_df.iterrows():\n",
    "        item_mappings_dict[int(_)] = row['item']\n",
    "    return item_mappings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"helper class for reading embeddings\"\"\"\n",
    "class Embeddings:\n",
    "    def __init__(self, item_embeddings,item_mapping_dict):\n",
    "        self.item_embeddings = item_embeddings\n",
    "        self.item_mapping_dict = item_mapping_dict\n",
    "        self.index_mapping_dict = {}\n",
    "        for key, val in self.item_mapping_dict.items():\n",
    "            self.index_mapping_dict[val] = key\n",
    "  \n",
    "    def size(self):\n",
    "        return self.item_embeddings.shape[1]\n",
    "\n",
    "    def get_embedding_vector(self):\n",
    "        return self.item_embeddings\n",
    "\n",
    "    def get_embedding(self, idx):\n",
    "        if isinstance(idx, str):\n",
    "            index = index_mapping_dict[idx]\n",
    "            return self.item_embeddings[index]\n",
    "        else:\n",
    "            return self.item_embeddings[idx]\n",
    "\n",
    "    def embed(self, item_list):\n",
    "        return np.array([self.get_embedding(idx) for idx,item in enumerate(item_list)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment/Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import Env, spaces\n",
    "from gym.utils import seeding\n",
    "from gym.envs.registration import register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module '__main__'>"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.modules[__name__]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_env():\n",
    "    register(id=\"recsys-v0\",entry_point = \"__main__:recsys\") #change module (__main__) when converting to python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "class recsys(Env):\n",
    "    def __init__(self,data,embeddings,alpha,gamma,fixed_length):\n",
    "        self.embeddings = embeddings\n",
    "        self.embedded_data = pd.DataFrame()\n",
    "        self.embedded_data['state'] = [np.array([embeddings.get_embedding(item_id)\n",
    "                                                for item_id in row['state']]) for _,row in data.iterrows()]\n",
    "        self.embedded_data['action'] = [np.array([embeddings.get_embedding(item_id) for item_id in row['action']])\n",
    "                                       for _,row in data.iterrows()]\n",
    "        self.embedded_data['reward'] = data['reward']\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.fixed_length = fixed_length\n",
    "        self.current_state = self.reset()\n",
    "        self.groups = self.get_groups()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.init_state = self.embedded_data['state'].sample(1).values[0]\n",
    "        return self.init_state\n",
    "    def step(self,actions):\n",
    "        '''\n",
    "        Compute reward and update state.\n",
    "        Args:\n",
    "          actions: embedded chosen items.\n",
    "        Returns:\n",
    "          cumulated_reward: overall reward.\n",
    "          current_state: updated state.\n",
    "        '''\n",
    "        #compute overall reward according to equation 4 in RL Listwise recommender paper\n",
    "        simulated_rewards, cumulative_rewards = self.simulate_rewards(self.current_state.reshape((1, -1)), actions.reshape((1, -1)))\n",
    "        #Simulator memory from algorithm 1\n",
    "        for k in range(len(simulated_rewards)):\n",
    "            if float(simulated_rewards[k]) > 0.0: #if positive reward then append action to the end of current state\n",
    "                self.current_state = np.append(self.current_state,[actions[k]],axis = 0)\n",
    "                if self.fixed_length:\n",
    "                    #remove the first item from current_state to keep the simulator memory constant\n",
    "                    self.current_state = np.delete(self.current_state,0,axis = 0)\n",
    "        return cumulative_rewards, self.current_state,False,{}\n",
    "       \n",
    "    def get_groups(self):\n",
    "        \"\"\"calculate average state action value for each group in dataframe rewards (eqn 3)\"\"\"\n",
    "        groups = []\n",
    "        for rewards,group in self.embedded_data.groupby(['reward']):\n",
    "            size = group.shape[0]\n",
    "            states = np.array(list(group['state'].values))\n",
    "            actions = np.array(list(group['action'].values))\n",
    "            groups.append({\n",
    "                'size':size, #Nx\n",
    "                'rewards': rewards, # U_x (combination of rewards)\n",
    "                'average state': (np.sum(states / np.linalg.norm(states, 2, axis=1)[:, np.newaxis], axis=0) / size).reshape((1, -1)), # s_x^-\n",
    "                'average action': (np.sum(actions / np.linalg.norm(actions, 2, axis=1)[:, np.newaxis], axis=0) / size).reshape((1, -1)) # a_x^-\n",
    "            })                \n",
    "        return groups\n",
    "    \n",
    "    def simulate_rewards(self,current_state,action): \n",
    "        #we'll assume only one reward type which is grouped cosine according to the RL paper\n",
    "        '''\n",
    "        Calculate simulated rewards.\n",
    "        Args:\n",
    "          current_state: history, list of embedded items.\n",
    "          action: embedded chosen item.\n",
    "        Returns:\n",
    "          returned_rewards: argmax of probable rewards\n",
    "          cumulated_reward: probability weighted rewards.\n",
    "        '''\n",
    "        def cosine_state_action(s_t, a_t, s_i, a_i):\n",
    "            #Calculate cosine similarity between (state,action) pair\n",
    "            cosine_state = np.dot(s_t, s_i.T) / (np.linalg.norm(s_t, 2) * np.linalg.norm(s_i, 2))\n",
    "            cosine_action = np.dot(a_t, a_i.T) / (np.linalg.norm(a_t, 2) * np.linalg.norm(a_i, 2))\n",
    "            return (self.alpha * cosine_state + (1 - self.alpha) * cosine_action).reshape((1,))\n",
    "        \n",
    "        #Calculate simulated rewards by grouped cosine (equation 1 and 3)\n",
    "        probabilities = [cosine_state_action(current_state, action, g['average state'], g['average action']) \n",
    "                         for g in self.groups]\n",
    "        #normalize probabilities to 1\n",
    "        probabilities = np.array(probabilities)/sum(probabilities)\n",
    "        returned_rewards = self.groups[np.argmax(probabilities)]['rewards']\n",
    "        def overall_reward(rewards,gamma):\n",
    "            return np.sum([(gamma**k) * float(reward) for k, reward in enumerate(rewards)])\n",
    "        # Get probability weighted cumulated reward\n",
    "        cumulated_reward = np.sum([p * overall_reward(g['rewards'], self.gamma) \n",
    "                                   for p, g in zip(probabilities, self.groups)])\n",
    "        \n",
    "        return returned_rewards, cumulated_reward\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Further steps -> coding actor and critic network, replay memory, train functions and evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy function approximator --> Actor Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor():\n",
    "    \"\"\"Policy Function Approximator\"\"\"\n",
    "    def __init__(self,session,state_space_size,action_space_size,batch_size,embedding_size,\\\n",
    "                 tau,actor_learning_rate,action_len=1,history_len=10,scope='actor'):\n",
    "        self.session = session\n",
    "        self.state_space_size = state_space_size\n",
    "        self.action_space_size = action_space_size\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.tau = tau\n",
    "        self.actor_lr = actor_learning_rate\n",
    "        self.action_len = action_len\n",
    "        self.history_len = history_len\n",
    "        self.scope = scope\n",
    "        with tf.compat.v1.variable_scope(self.scope):\n",
    "            #Build estimator actor network\n",
    "            self.action_weights,self.state,self.sequence_length = self.build_net('estimate_actor')\n",
    "            self.network_params = tf.compat.v1.trainable_variables()\n",
    "            #Build target network\n",
    "            self.target_action_weights,self.target_state,self.target_sequence_len = self.build_net('target_actor')\n",
    "            #get network parameters for target_actor network\n",
    "            self.target_network_params =tf.compat.v1.trainable_variables()[len(self.network_params):]\n",
    "\n",
    "            # Initialize target network weights with network weights (θ^π′ ← θ^π)\n",
    "            self.init_target_network_params = [self.target_network_params[i].assign(self.network_params[i])\n",
    "            for i in range(len(self.target_network_params))]\n",
    "\n",
    "            # Update target network weights (θ^π′ ← τθ^π + (1 − τ)θ^π′)\n",
    "            self.update_target_network_params = [self.target_network_params[i].assign(\n",
    "                tf.multiply(self.tau, self.network_params[i]) +\n",
    "                tf.multiply(1 - self.tau, self.target_network_params[i])) \n",
    "                                                for i in range(len(self.target_network_params))]\n",
    "\n",
    "            self.action_gradient = tf.compat.v1.placeholder(tf.float32,[None,self.action_space_size])\n",
    "            gradients = tf.gradients(tf.reshape(self.action_weights,[self.batch_size,self.action_space_size]),\\\n",
    "                                     self.network_params,self.action_gradient)\n",
    "            self.params_gradient = list(map(\n",
    "                lambda x: tf.compat.v1.div(x,self.batch_size * self.action_space_size),gradients\n",
    "            ))\n",
    "            # Compute ∇_a.Q(s, a|θ^µ).∇_θ^π.f_θ^π(s)\n",
    "            self.optimizer = tf.compat.v1.train.AdamOptimizer(self.actor_lr).apply_gradients(\n",
    "                zip(self.params_gradient, self.network_params)\n",
    "            )\n",
    "            self.num_trainable_vars = len(self.network_params) + len(self.target_network_params)\n",
    "    \n",
    "\n",
    "    def build_net(self,scope):\n",
    "        \"\"\"Build Tensorflow Graph\"\"\"\n",
    "        def gather_last_output(data, seq_lens):\n",
    "            def cli_value(x, v):\n",
    "                y = tf.constant(v, shape=x.get_shape(), dtype=tf.int64)\n",
    "                x = tf.cast(x, tf.int64)\n",
    "                return tf.where(tf.greater(x, y), x, y)\n",
    "            batch_range = tf.range(tf.cast(tf.shape(data)[0], dtype=tf.int64), dtype=tf.int64)\n",
    "            tmp_end = tf.map_fn(lambda x: cli_value(x, 0), seq_lens - 1, dtype=tf.int64)\n",
    "            indices = tf.stack([batch_range, tmp_end], axis=1)\n",
    "            return tf.gather_nd(data, indices)  \n",
    "        \n",
    "        with tf.compat.v1.variable_scope(scope):\n",
    "            state = tf.compat.v1.placeholder(tf.float32,[None,self.state_space_size],\"state\")\n",
    "            state_ = tf.reshape(state,[-1,self.history_len,self.embedding_size])\n",
    "            sequence_length = tf.compat.v1.placeholder(tf.int32,[None],'sequence_length')\n",
    "            cell = tf.compat.v1.nn.rnn_cell.GRUCell(self.embedding_size,\n",
    "                                                   activation = tf.nn.relu,\n",
    "                                                   kernel_initializer = tf.initializers.random_normal(),\n",
    "                                                   bias_initializer = tf.zeros_initializer())\n",
    "            outputs,_ = tf.compat.v1.nn.dynamic_rnn(cell,state_,dtype=tf.float32,sequence_length=sequence_length)\n",
    "            last_output = gather_last_output(outputs,sequence_length)\n",
    "            x = tf.keras.layers.Dense(self.action_len * self.embedding_size)(last_output)\n",
    "            action_weights = tf.reshape(x,[-1,self.action_len,self.embedding_size])\n",
    "        return action_weights, state, sequence_length\n",
    "    def train(self,state,sequence_length,action_gradients):\n",
    "        \"\"\"\n",
    "      Compute ∇_a.Q(s, a|θ^µ).∇_θ^π.f_θ^π(s)\n",
    "        \"\"\"\n",
    "        self.session.run(self.optimizer,feed_dict={\n",
    "            self.state:state,\n",
    "            self.sequence_length:sequence_length,\n",
    "            self.action_gradient:action_gradients\n",
    "        })\n",
    "        \n",
    "    def predict(self,state,sequence_length):\n",
    "        return self.session.run(self.action_weights,feed_dict={\n",
    "            self.state:state,\n",
    "            self.sequence_length:sequence_length\n",
    "        })\n",
    "    def predict_target(self,state,sequence_length):\n",
    "        return self.session.run(self.target_action_weights,feed_dict={\n",
    "            self.target_state: state,\n",
    "            self.target_sequence_len:sequence_length\n",
    "        })\n",
    "    def init_target_network(self):\n",
    "        self.session.run(self.init_target_network_params)\n",
    "    def update_target_network(self):\n",
    "        self.session.run(self.update_target_network_params)\n",
    "    def get_num_trainable_vars(self):\n",
    "        return self.num_trainable_vars\n",
    "    \n",
    "    def get_recommendation(self,action_len,noisy_state,embeddings,target=False):\n",
    "        \"\"\"\n",
    "        Algorithm 2 from Listwise Recommendation Paper\n",
    "        Args:\n",
    "        action_len: length of recommendation list (K)\n",
    "        noisy_state: environment state with noise\n",
    "        embeddings: Embeddings class object\n",
    "        target: boolean to indicate use of Actor network or Target Network\n",
    "        \n",
    "        Returns:\n",
    "        Recommendation: list of embedded item as future actions\n",
    "        \"\"\"\n",
    "        def get_score(weights,embedding,batch_size):\n",
    "            return np.dot(weights,embedding.T)\n",
    "        \n",
    "        batch_size = noisy_state.shape[0]\n",
    "        method = self.predict_target if target else self.predict\n",
    "        weights = method(noisy_state,[action_len]*batch_size)\n",
    "        \n",
    "        scores = np.array([[[get_score(weights[i][j],embedding,batch_size)\n",
    "                            for embedding in embeddings.get_embedding_vector()]\n",
    "                           for j in range(action_len)]\n",
    "                          for i in range(batch_size)])\n",
    "        return np.array([[embeddings.get_embedding(np.argmax(scores[i][j]))\n",
    "                         for j in range(action_len)]\n",
    "                        for i in range(batch_size)])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value function approximator --> Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic():\n",
    "    \"\"\"Value Function Approximator\"\"\"\n",
    "    def __init__(self,session,state_space_size,action_space_size,embedding_size,\\\n",
    "                 tau,critic_learning_rate,history_len=10,scope='critic'):\n",
    "        self.session = session\n",
    "        self.state_space_size = state_space_size\n",
    "        self.action_space_size = action_space_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.tau = tau\n",
    "        self.learning_rate = critic_learning_rate\n",
    "        self.history_len = history_len\n",
    "        self.scope = scope\n",
    "        with tf.compat.v1.variable_scope(self.scope):\n",
    "            #Build critic Network\n",
    "            self.critic_Q_value,self.state,self.action,self.sequence_length = self.build_net('estimator_critic')\n",
    "            self.network_params = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES,\\\n",
    "                                                              scope='estimator_critic')\n",
    "            \n",
    "            #Build target Critic Network\n",
    "            self.target_Q_value,self.target_state,self.target_action,\\\n",
    "            self.target_sequence_length = self.build_net('target_critic')\n",
    "            self.target_network_params = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES,\\\n",
    "                                                           scope='target_critic')\n",
    "            \n",
    "            #Initialize target network weights with network weights\n",
    "            self.init_target_network_params = [self.target_network_params[i].assign(self.network_params[i]) \n",
    "                                               for i in range(len(self.target_network_params))]\n",
    "            #Update Target network weights\n",
    "            self.update_target_network_params = [self.target_network_params[i].assign(\n",
    "                tf.multiply(self.tau, self.network_params[i]) + \n",
    "                tf.multiply(1 - self.tau, self.target_network_params[i]))\n",
    "                for i in range(len(self.target_network_params))]\n",
    "            \n",
    "            #Minimize MSE between critic's Q values and target critic's output Q values\n",
    "            self.expected_reward = tf.compat.v1.placeholder(tf.float32,[None,1])\n",
    "            self.loss = tf.reduce_mean(tf.math.squared_difference(self.expected_reward,self.critic_Q_value))\n",
    "            self.optimizer = tf.compat.v1.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "            \n",
    "            #Compute action gradients  ∇_a.Q(s, a|θ^µ)\n",
    "            self.action_gradients = tf.gradients(self.critic_Q_value,self.action)\n",
    "    \n",
    "    def build_net(self,scope):\n",
    "        # Inputs: current state, current action\n",
    "        # Outputs: predicted Q-value\n",
    "        \n",
    "        def gather_last_output(data, seq_lens):\n",
    "            def cli_value(x, v):\n",
    "                y = tf.constant(v, shape=x.get_shape(), dtype=tf.int64)\n",
    "                return tf.where(tf.greater(x, y), x, y)\n",
    "            this_range = tf.range(tf.cast(tf.shape(seq_lens)[0], dtype=tf.int64), dtype=tf.int64)\n",
    "            tmp_end = tf.map_fn(lambda x: cli_value(x, 0), seq_lens - 1, dtype=tf.int64)\n",
    "            indices = tf.stack([this_range, tmp_end], axis=1)\n",
    "            return tf.gather_nd(data, indices)\n",
    "\n",
    "        with tf.compat.v1.variable_scope(scope):\n",
    "            state = tf.compat.v1.placeholder(tf.float32,[None,self.state_space_size],'state')\n",
    "            state_ = tf.reshape(state, [-1, self.history_len, self.embedding_size])\n",
    "            action = tf.compat.v1.placeholder(tf.float32, [None, self.action_space_size], 'action')\n",
    "            sequence_length = tf.compat.v1.placeholder(tf.int64, [None], name='critic_sequence_length')\n",
    "            cell = tf.compat.v1.nn.rnn_cell.GRUCell(self.history_len,\n",
    "                                        activation=tf.nn.relu,\n",
    "                                        kernel_initializer=tf.initializers.random_normal(),\n",
    "                                        bias_initializer=tf.zeros_initializer())\n",
    "            predicted_state, _ = tf.compat.v1.nn.dynamic_rnn(cell, state_, dtype=tf.float32, sequence_length=sequence_length)\n",
    "            predicted_state = gather_last_output(predicted_state, sequence_length)\n",
    "            \n",
    "            inputs = tf.concat([predicted_state, action], axis=-1)\n",
    "            layer1 = tf.keras.layers.Dense(32, activation=tf.nn.relu)(inputs)\n",
    "            layer2 = tf.keras.layers.Dense(16, activation=tf.nn.relu)(layer1)\n",
    "            critic_Q_value = tf.keras.layers.Dense(1)(layer2)\n",
    "        return critic_Q_value, state, action, sequence_length            \n",
    "        \n",
    "    def train(self,state,action,sequence_length,expected_reward):\n",
    "        \"\"\"\n",
    "        MINIMIZE MSE between target critic Q val and expected reward \n",
    "        \"\"\"\n",
    "        return self.session.run([self.critic_Q_value,self.loss,self.optimizer],\n",
    "                            feed_dict={\n",
    "                                self.state:state,\n",
    "                                self.action:action,\n",
    "                                self.sequence_length:sequence_length,\n",
    "                                self.expected_reward:expected_reward\n",
    "                            })\n",
    "    def predict(self,state,action,sequence_length):\n",
    "        \"\"\"\n",
    "        Return critic's predicted Q val\n",
    "        \"\"\"\n",
    "        return self.session.run(self.critic_Q_value,\n",
    "                               feed_dict={\n",
    "                                   self.state:state,\n",
    "                                   self.action:action,\n",
    "                                   self.sequence_length: sequence_length\n",
    "                               })\n",
    "    def predict_target(self, state, action, sequence_length):\n",
    "        \"\"\" \n",
    "        Returns target Critic's predicted Q-value. \n",
    "        \"\"\"\n",
    "        return self.session.run(self.target_Q_value,\n",
    "                             feed_dict={\n",
    "                                 self.target_state: state,\n",
    "                                 self.target_action: action,\n",
    "                                 self.target_sequence_length: sequence_length\n",
    "                             })\n",
    "    def get_action_gradients(self, state, action, sequence_length):\n",
    "        \"\"\"\n",
    "        Returns ∇_a.Q(s,a|θ^µ)\n",
    "        \"\"\"\n",
    "        return np.array(self.session.run(self.action_gradients,\n",
    "                             feed_dict={\n",
    "                                 self.state: state,\n",
    "                                 self.action: action,\n",
    "                                 self.sequence_length: sequence_length\n",
    "                             })[0])\n",
    "    \n",
    "    def init_target_network(self):\n",
    "        self.session.run(self.init_target_network_params)\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        self.session.run(self.update_target_network_params)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    \"\"\"\n",
    "    Replay Memory D in Listwise Recommendation Paper\n",
    "    \"\"\"\n",
    "    def __init__(self,buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = []\n",
    "    def add(self,state,action,reward,n_state):\n",
    "        self.buffer.append([state,action,reward,n_state])\n",
    "        if len(self.buffer) > self.buffer_size:\n",
    "            self.buffer.pop(0)\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "    def sample_batch(self,batch_size):\n",
    "        return random.sample(self.buffer,batch_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experience_replay(replay_memory,batch_size,actor,critic,\\\n",
    "                      embeddings,action_len,state_space_size,action_space_size,gamma):\n",
    "    \"\"\"\n",
    "    Experience Replay\n",
    "Args:\n",
    "    replay_memory: replay_memory class object\n",
    "    batch_size: sample_size\n",
    "    actor: Actor Network\n",
    "    critic: Critic Network\n",
    "    embeddings: Embeddings class object\n",
    "    state_space_size: dimension of states\n",
    "    action_space_size: dimension of actions\n",
    "Returns:\n",
    "    Best Q-value, loss of critic network\n",
    "    \"\"\"\n",
    "    samples = replay_memory.sample_batch(batch_size)\n",
    "    states = np.array([s[0] for s in samples])\n",
    "    actions = np.array([s[1] for s in samples])\n",
    "    rewards = np.array([s[2] for s in samples])\n",
    "    n_states = np.array([s[3] for s in samples]).reshape(-1, state_space_size)\n",
    "\n",
    "  # '23: Generate a′ by target Actor network according to Algorithm 2'\n",
    "    n_actions = actor.get_recommendation(action_len, states, embeddings, target=True).reshape(-1, action_space_size)\n",
    "\n",
    "  # Calculate predicted Q′(s′, a′|θ^µ′) value\n",
    "    target_Q_value = critic.predict_target(n_states, n_actions, [action_len] * batch_size)\n",
    "\n",
    "  # '24: Set y = r + γQ′(s′, a′|θ^µ′)'\n",
    "    expected_rewards = rewards + gamma * target_Q_value\n",
    "  \n",
    "  # '25: Update Critic by minimizing (y − Q(s, a|θ^µ))²'\n",
    "    critic_Q_value, critic_loss, _ = critic.train(states, actions, [action_len] * batch_size, expected_rewards)\n",
    "  \n",
    "  # '26: Update the Actor using the sampled policy gradient'\n",
    "    action_gradients = critic.get_action_gradients(states, n_actions, [action_len] * batch_size)\n",
    "    actor.train(states, [action_len] * batch_size, action_gradients)\n",
    "\n",
    "  # '27: Update the Critic target networks'\n",
    "    critic.update_target_network()\n",
    "\n",
    "  # '28: Update the Actor target network'\n",
    "    actor.update_target_network()\n",
    "\n",
    "    return np.amax(critic_Q_value), critic_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrnsteinUhlenbeckNoise:\n",
    "    \"\"\"Noise for Actor Predictions\"\"\"\n",
    "    def __init__(self,action_space_size,mu=0,theta=0.5,sigma=0.2):\n",
    "        self.action_space_size = action_space_size\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = np.ones(self.action_space_size) * self.mu\n",
    "    def get(self):\n",
    "        self.state += self.theta * (self.mu - self.state) + self.sigma*np.random.rand(self.action_space_size)\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(session,env,actor,critic,embeddings,history_len,action_len,buffer_size,batch_size,\\\n",
    "         gamma,nb_episodes,nb_rounds,filename_summary):\n",
    "    \"\"\"Algorithm 3 in Listwise Recommendation Paper\"\"\"\n",
    "    session.run(tf.compat.v1.global_variables_initializer())    \n",
    "\n",
    "    actor.init_target_network()\n",
    "    critic.init_target_network()\n",
    "    \n",
    "    replay_memory = ReplayMemory(buffer_size)\n",
    "    replay = False\n",
    "    \n",
    "    start_time = time.time()\n",
    "    summary_dict = {}\n",
    "    for i_session in range(nb_episodes):\n",
    "        session_reward = 0\n",
    "        session_Q_value = 0\n",
    "        session_critic_loss = 0\n",
    "        summary_dict[i_session] = {}\n",
    "        summary_dict[i_session][\"session_reward\"] = []\n",
    "        summary_dict[i_session][\"session_Q_value\"] = []\n",
    "        summary_dict[i_session][\"session_critic_loss\"] = []\n",
    "        states = env.reset()#Initialize state s0 from previous sessions\n",
    "        if (i_session + 1) % 10 == 0: #Update average parameters every 10 episodes\n",
    "            env.groups = env.get_groups()\n",
    "        \n",
    "        \n",
    "        for t in tqdm.trange(nb_rounds):\n",
    "            #select actions according to get recommendation list\n",
    "            exploration_noise = OrnsteinUhlenbeckNoise(len(states) * embeddings.size())\n",
    "            actions = actor.get_recommendation(action_len,\\\n",
    "                                               states.reshape(1,-1) + exploration_noise.get().reshape(1,-1),\n",
    "                                               embeddings\n",
    "                                              ).reshape(action_len,embeddings.size())\n",
    "            rewards,next_states,done,_ = env.step(actions)\n",
    "            \n",
    "            replay_memory.add(states.reshape(history_len * embeddings.size()),\n",
    "                             actions.reshape(action_len * embeddings.size()),\n",
    "                             [rewards],\n",
    "                             next_states.reshape(len(next_states)*embeddings.size()))\n",
    "            states = next_states\n",
    "            session_reward += rewards\n",
    "            \n",
    "            #parameter update\n",
    "            if replay_memory.size() >= batch_size:\n",
    "                replay = True\n",
    "                replay_Q_value, critic_loss = experience_replay(replay_memory,batch_size,\n",
    "                                                               actor,critic,embeddings,action_len,\\\n",
    "                                                               history_len * embeddings.size(),\n",
    "                                                               action_len * embeddings.size(),\n",
    "                                                               gamma)\n",
    "                session_Q_value += replay_Q_value\n",
    "                session_critic_loss += critic_loss\n",
    "                summary_dict[i_session][\"session_Q_value\"].append(session_Q_value)\n",
    "                summary_dict[i_session][\"session_critic_loss\"].append(session_critic_loss)                \n",
    "            summary_dict[i_session][\"session_reward\"].append(session_reward)\n",
    "\n",
    "        \n",
    "        \n",
    "#         print(\"Session Reward: {}, Session_Q_value: {}, Session_critic_loss: {}\".format(session_reward,\n",
    "#                                                                                        session_Q_value,\n",
    "#                                                                                        session_critic_loss))\n",
    "\n",
    "        str_loss = str('Loss=%0.4f' % session_critic_loss)\n",
    "        print(('Episode %d/%d Reward=%d Time=%ds ' + (str_loss if replay else 'No replay')) % (i_session + 1, nb_episodes, session_reward, time.time() - start_time))\n",
    "        start_time = time.time()\n",
    "    with open(filename_summary, 'wb') as f:\n",
    "        pickle.dump(summary_dict, f)\n",
    "    tf.compat.v1.train.Saver().save(session,'models.h5',write_meta_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparams\n",
    "history_len = 10\n",
    "action_len = 1\n",
    "discount_factor = 0.99\n",
    "actor_lr = 0.0001\n",
    "critic_lr = 0.001\n",
    "tau = 0.001\n",
    "batch_size = 64\n",
    "nb_episodes = 100\n",
    "nb_rounds = 50\n",
    "filename_summary = 'summary.pkl'\n",
    "alpha = 0.5\n",
    "gamma = 0.9\n",
    "buffer_size = 1000000\n",
    "fixed_len = True\n",
    "state_space_size = embeddings.size() * history_len\n",
    "action_space_size = embeddings.size() * action_len\n",
    "data = file_convert(sequence_df.copy())\n",
    "train, test = train_test_split(data, test_size=0.25,random_state=0)\n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test code\n",
    "item_map = create_item_mappings(embeddings_df)\n",
    "embeddings = Embeddings(read_embeddings(embeddings_df),item_map)\n",
    "register_env()\n",
    "env = gym.make('recsys-v0',data = data,embeddings=embeddings,alpha=alpha,gamma=gamma,fixed_length=fixed_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-03 15:57:25.642279: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2021-12-03 15:57:25.642371: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "session = tf.compat.v1.Session()\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "# Initialize actor network f_θ^π and critic network Q(s, a|θ^µ) with random weights\n",
    "actor = Actor(session, state_space_size, action_space_size, batch_size, embeddings.size(),tau,actor_lr,\\\n",
    "              action_len, history_len)\n",
    "critic = Critic(session, state_space_size, action_space_size, embeddings.size(),\\\n",
    "                tau, critic_lr,history_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-03 15:57:27.514391: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-12-03 15:57:27.751654: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "  0%|                                                                                                                                            | 0/50 [00:00<?, ?it/s]2021-12-03 15:57:27.801665: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 24.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100 Reward=150 Time=2s No replay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██████████████████████████████████                                                                                                 | 13/50 [00:00<00:01, 29.49it/s]2021-12-03 15:57:30.361129: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-12-03 15:57:32.190865: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-12-03 15:57:32.565771: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-12-03 15:57:33.225180: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-12-03 15:57:33.645050: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-12-03 15:57:34.188145: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [01:41<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2/100 Reward=150 Time=101s Loss=161.8500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [02:11<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3/100 Reward=149 Time=131s Loss=15.3950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [02:10<00:00,  2.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4/100 Reward=99 Time=130s Loss=197.7175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [02:15<00:00,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5/100 Reward=150 Time=135s Loss=325.0724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎               | 44/50 [01:59<00:18,  3.05s/it]"
     ]
    }
   ],
   "source": [
    "train_network(session, env, actor, critic, embeddings, history_len, action_len,\\\n",
    "      buffer_size,batch_size,discount_factor,nb_episodes,nb_rounds,filename_summary=filename_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('summary.pkl', 'rb') as f:\n",
    "        loaded_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
