{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/sample_df.csv')\n",
    "sequence_df = pd.read_csv('data/preprocessed_df.csv')\n",
    "embeddings_df = pd.read_csv('data/embeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_convert(sequence_data):\n",
    "    \"\"\"Seperate ratings and items\"\"\"\n",
    "    for col in ['state', 'next_state', 'action_reward']:\n",
    "        sequence_data[col] = [np.array([[k for k in ee.split('&')] for ee in e.split('|')]) for e in sequence_data[col]]\n",
    "    for col in ['state', 'next_state']:\n",
    "        sequence_data[col] = [np.array([e[0] for e in l]) for l in sequence_data[col]]\n",
    "\n",
    "    sequence_data['action'] = [[e[0] for e in l] for l in sequence_data['action_reward']]\n",
    "    sequence_data['reward'] = [tuple(e[1] for e in l) for l in sequence_data['action_reward']]\n",
    "    sequence_data.drop(columns=['action_reward'], inplace=True)\n",
    "    return sequence_data   \n",
    "def read_embeddings(embeddings_file):\n",
    "    '''Load embeddings (a vector for each item).''' \n",
    "    return np.array([[np.float64(k) for k in e.split('|')]\n",
    "                   for e in embeddings_file['embedding']])\n",
    "def create_item_mappings(embeddings_df):\n",
    "    \"\"\"since items are strings, map them with integer index\"\"\"\n",
    "    item_mappings_dict = {}\n",
    "    for _,row in embeddings_df.iterrows():\n",
    "        item_mappings_dict[row['item']] = int(_)\n",
    "    return item_mappings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"helper class for reading embeddings\"\"\"\n",
    "class Embeddings:\n",
    "    def __init__(self, item_embeddings,item_mapping_dict):\n",
    "        self.item_embeddings = item_embeddings\n",
    "        self.item_mapping_dict = item_mapping_dict\n",
    "  \n",
    "    def size(self):\n",
    "        return self.item_embeddings.shape[1]\n",
    "\n",
    "    def get_embedding_vector(self):\n",
    "        return self.item_embeddings\n",
    "\n",
    "    def get_embedding(self, item):\n",
    "        item_index = self.item_mapping_dict[item]\n",
    "        return self.item_embeddings[item_index]\n",
    "\n",
    "    def embed(self, item_list):\n",
    "        return np.array([self.get_embedding(item) for item in item_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment/Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import Env, spaces\n",
    "from gym.utils import seeding\n",
    "from gym.envs.registration import register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module '__main__'>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.modules[__name__]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_env():\n",
    "    register(id=\"recsys-v0\",entry_point = \"__main__:recsys\") #change module (__main__) when converting to python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class recsys(Env):\n",
    "    def __init__(self,data,embeddings,alpha,gamma,fixed_length):\n",
    "        self.embeddings = embeddings\n",
    "        self.embedded_data = pd.DataFrame()\n",
    "        self.embedded_data['state'] = [np.array([embeddings.get_embedding(item_id)\n",
    "                                                for item_id in row['state']]) for _,row in data.iterrows()]\n",
    "        self.embedded_data['action'] = [np.array([embeddings.get_embedding(item_id) for item_id in row['action']])\n",
    "                                       for _,row in data.iterrows()]\n",
    "        self.embedded_data['reward'] = data['reward']\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.fixed_length = fixed_length\n",
    "        self.current_state = self.reset()\n",
    "        self.groups = self.get_groups\n",
    "    \n",
    "    def reset(self):\n",
    "        self.init_state = self.embedded_data['state'].sample(1).values[0]\n",
    "    def step(self,actions):\n",
    "        '''\n",
    "        Compute reward and update state.\n",
    "        Args:\n",
    "          actions: embedded chosen items.\n",
    "        Returns:\n",
    "          cumulated_reward: overall reward.\n",
    "          current_state: updated state.\n",
    "        '''\n",
    "        #compute overall reward according to equation 4 in RL Listwise recommender paper\n",
    "        simulated_rewards, cumulative_rewards = self.simulate_rewards(self.current_state.reshape((1, -1)), actions.reshape((1, -1)))\n",
    "        #Simulator memory from algorithm 1\n",
    "        for k in range(len(simulated_rewards)):\n",
    "            if simulated_rewards[k] > 0: #if positive reward then append action to the end of current state\n",
    "                self.current_state = np.append(self.current_state,actions[k],axis = 0)\n",
    "                if self.fixed_length:\n",
    "                    #remove the first item from current_state to keep the simulator memory constant\n",
    "                    self.current_state = np.delete(self.current_state,0,axis = 0)\n",
    "        return cumulative_rewards, self.current_state\n",
    "       \n",
    "    def get_groups(self):\n",
    "        \"\"\"calculate average state action value for each group in dataframe rewards (eqn 3)\"\"\"\n",
    "        groups = []\n",
    "        for rewards,group in self.embedded_data.groupby(['reward']):\n",
    "            size = group.shape[0]\n",
    "            states = np.array(list(group['state'].values))\n",
    "            actions = np.array(list(group['action'].values))\n",
    "            groups.append({\n",
    "                'size':size, #Nx\n",
    "                'rewards': rewards, # U_x (combination of rewards)\n",
    "                'average state': (np.sum(states / np.linalg.norm(states, 2, axis=1)[:, np.newaxis], axis=0) / size).reshape((1, -1)), # s_x^-\n",
    "                'average action': (np.sum(actions / np.linalg.norm(actions, 2, axis=1)[:, np.newaxis], axis=0) / size).reshape((1, -1)) # a_x^-\n",
    "            })                \n",
    "        return groups\n",
    "    \n",
    "    def simulate_rewards(self,current_state,action): \n",
    "        #we'll assume only one reward type which is grouped cosine according to the RL paper\n",
    "        '''\n",
    "        Calculate simulated rewards.\n",
    "        Args:\n",
    "          current_state: history, list of embedded items.\n",
    "          action: embedded chosen item.\n",
    "        Returns:\n",
    "          returned_rewards: argmax of probable rewards\n",
    "          cumulated_reward: probability weighted rewards.\n",
    "        '''\n",
    "        def cosine_state_action(s_t, a_t, s_i, a_i):\n",
    "            #Calculate cosine similarity between (state,action) pair\n",
    "            cosine_state = np.dot(s_t, s_i.T) / (np.linalg.norm(s_t, 2) * np.linalg.norm(s_i, 2))\n",
    "            cosine_action = np.dot(a_t, a_i.T) / (np.linalg.norm(a_t, 2) * np.linalg.norm(a_i, 2))\n",
    "        return (self.alpha * cosine_state + (1 - self.alpha) * cosine_action).reshape((1,))\n",
    "        \n",
    "        #Calculate simulated rewards by grouped cosine (equation 1 and 3)\n",
    "        probabilities = [cosine_state_action(current_state, action, g['average state'], g['average action']) \n",
    "                         for g in self.groups]\n",
    "        #normalize probabilities to 1\n",
    "        probabilities = np.array(probabilities)/sum(probabilities)\n",
    "        returned_rewards = self.groups[np.argmax(probabilities)]['rewards']\n",
    "        def overall_reward(rewards,gamma):\n",
    "            return np.sum([gamma**k * reward for k, reward in enumerate(rewards)])\n",
    "        # Get probability weighted cumulated reward\n",
    "        cumulated_reward = np.sum([p * overall_reward(g['rewards'], self.gamma) \n",
    "                                   for p, g in zip(probabilities, self.groups)])\n",
    "        \n",
    "        return returned_rewards, cumulated_reward\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Test code\n",
    "item_map = create_item_mappings(embeddings_df)\n",
    "embeddings = Embeddings(read_embeddings(embeddings_df),item_map)\n",
    "data = file_convert(sequence_df.copy())\n",
    "register_env()\n",
    "env = gym.make('recsys-v0',data=data,embeddings=embeddings,alpha=0.5,gamma=1,fixed_length=10) #random parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Further steps -> coding actor and critic network, replay memory, train functions and evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
