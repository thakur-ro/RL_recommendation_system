{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/sample_df.csv')\n",
    "sequence_df = pd.read_csv('data/preprocessed_df.csv')\n",
    "embeddings_df = pd.read_csv('data/embeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_convert(sequence_data):\n",
    "    \"\"\"Seperate ratings and items\"\"\"\n",
    "    for col in ['state', 'next_state', 'action_reward']:\n",
    "        sequence_data[col] = [np.array([[k for k in ee.split('&')] for ee in e.split('|')]) for e in sequence_data[col]]\n",
    "    for col in ['state', 'next_state']:\n",
    "        sequence_data[col] = [np.array([e[0] for e in l]) for l in sequence_data[col]]\n",
    "\n",
    "    sequence_data['action'] = [[e[0] for e in l] for l in sequence_data['action_reward']]\n",
    "    sequence_data['reward'] = [tuple(e[1] for e in l) for l in sequence_data['action_reward']]\n",
    "    sequence_data.drop(columns=['action_reward'], inplace=True)\n",
    "    return sequence_data   \n",
    "def read_embeddings(embeddings_file):\n",
    "    '''Load embeddings (a vector for each item).''' \n",
    "    return np.array([[np.float64(k) for k in e.split('|')]\n",
    "                   for e in embeddings_file['embedding']])\n",
    "def create_item_mappings(embeddings_df):\n",
    "    \"\"\"since items are strings, map them with integer index\"\"\"\n",
    "    item_mappings_dict = {}\n",
    "    for _,row in embeddings_df.iterrows():\n",
    "        item_mappings_dict[int(_)] = row['item']\n",
    "    return item_mappings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"helper class for reading embeddings\"\"\"\n",
    "class Embeddings:\n",
    "    def __init__(self, item_embeddings,item_mapping_dict):\n",
    "        self.item_embeddings = item_embeddings\n",
    "        self.item_mapping_dict = item_mapping_dict\n",
    "        self.index_mapping_dict = {}\n",
    "        for key, val in self.item_mapping_dict.items():\n",
    "            self.index_mapping_dict[val] = key\n",
    "  \n",
    "    def size(self):\n",
    "        return self.item_embeddings.shape[1]\n",
    "\n",
    "    def get_embedding_vector(self):\n",
    "        return self.item_embeddings\n",
    "\n",
    "    def get_embedding(self, idx):\n",
    "        if isinstance(idx, str):\n",
    "            index = index_mapping_dict[idx]\n",
    "            return self.item_embeddings[index]\n",
    "        else:\n",
    "            return self.item_embeddings[idx]\n",
    "\n",
    "    def embed(self, item_list):\n",
    "        return np.array([self.get_embedding(idx) for idx,item in enumerate(item_list)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment/Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import Env, spaces\n",
    "from gym.utils import seeding\n",
    "from gym.envs.registration import register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module '__main__'>"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.modules[__name__]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_env():\n",
    "    register(id=\"recsys-v0\",entry_point = \"__main__:recsys\") #change module (__main__) when converting to python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "class recsys(Env):\n",
    "    def __init__(self,data,embeddings,alpha,gamma,fixed_length):\n",
    "        self.embeddings = embeddings\n",
    "        self.embedded_data = pd.DataFrame()\n",
    "        self.embedded_data['state'] = [np.array([embeddings.get_embedding(item_id)\n",
    "                                                for item_id in row['state']]) for _,row in data.iterrows()]\n",
    "        self.embedded_data['action'] = [np.array([embeddings.get_embedding(item_id) for item_id in row['action']])\n",
    "                                       for _,row in data.iterrows()]\n",
    "        self.embedded_data['reward'] = data['reward']\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.fixed_length = fixed_length\n",
    "        self.current_state = self.reset()\n",
    "        self.groups = self.get_groups()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.init_state = self.embedded_data['state'].sample(1).values[0]\n",
    "        return self.init_state\n",
    "    def step(self,actions):\n",
    "        '''\n",
    "        Compute reward and update state.\n",
    "        Args:\n",
    "          actions: embedded chosen items.\n",
    "        Returns:\n",
    "          cumulated_reward: overall reward.\n",
    "          current_state: updated state.\n",
    "        '''\n",
    "        #compute overall reward according to equation 4 in RL Listwise recommender paper\n",
    "        simulated_rewards, cumulative_rewards = self.simulate_rewards(self.current_state.reshape((1, -1)), actions.reshape((1, -1)))\n",
    "        #Simulator memory from algorithm 1\n",
    "        for k in range(len(simulated_rewards)):\n",
    "            if float(simulated_rewards[k]) > 0.0: #if positive reward then append action to the end of current state\n",
    "                self.current_state = np.append(self.current_state,[actions[k]],axis = 0)\n",
    "                if self.fixed_length:\n",
    "                    #remove the first item from current_state to keep the simulator memory constant\n",
    "                    self.current_state = np.delete(self.current_state,0,axis = 0)\n",
    "        return cumulative_rewards, self.current_state,False,{}\n",
    "       \n",
    "    def get_groups(self):\n",
    "        \"\"\"calculate average state action value for each group in dataframe rewards (eqn 3)\"\"\"\n",
    "        groups = []\n",
    "        for rewards,group in self.embedded_data.groupby(['reward']):\n",
    "            size = group.shape[0]\n",
    "            states = np.array(list(group['state'].values))\n",
    "            actions = np.array(list(group['action'].values))\n",
    "            groups.append({\n",
    "                'size':size, #Nx\n",
    "                'rewards': rewards, # U_x (combination of rewards)\n",
    "                'average state': (np.sum(states / np.linalg.norm(states, 2, axis=1)[:, np.newaxis], axis=0) / size).reshape((1, -1)), # s_x^-\n",
    "                'average action': (np.sum(actions / np.linalg.norm(actions, 2, axis=1)[:, np.newaxis], axis=0) / size).reshape((1, -1)) # a_x^-\n",
    "            })                \n",
    "        return groups\n",
    "    \n",
    "    def simulate_rewards(self,current_state,action): \n",
    "        #we'll assume only one reward type which is grouped cosine according to the RL paper\n",
    "        '''\n",
    "        Calculate simulated rewards.\n",
    "        Args:\n",
    "          current_state: history, list of embedded items.\n",
    "          action: embedded chosen item.\n",
    "        Returns:\n",
    "          returned_rewards: argmax of probable rewards\n",
    "          cumulated_reward: probability weighted rewards.\n",
    "        '''\n",
    "        def cosine_state_action(s_t, a_t, s_i, a_i):\n",
    "            #Calculate cosine similarity between (state,action) pair\n",
    "            cosine_state = np.dot(s_t, s_i.T) / (np.linalg.norm(s_t, 2) * np.linalg.norm(s_i, 2))\n",
    "            cosine_action = np.dot(a_t, a_i.T) / (np.linalg.norm(a_t, 2) * np.linalg.norm(a_i, 2))\n",
    "            return (self.alpha * cosine_state + (1 - self.alpha) * cosine_action).reshape((1,))\n",
    "        \n",
    "        #Calculate simulated rewards by grouped cosine (equation 1 and 3)\n",
    "        probabilities = [cosine_state_action(current_state, action, g['average state'], g['average action']) \n",
    "                         for g in self.groups]\n",
    "        #normalize probabilities to 1\n",
    "        probabilities = np.array(probabilities)/sum(probabilities)\n",
    "        returned_rewards = self.groups[np.argmax(probabilities)]['rewards']\n",
    "        def overall_reward(rewards,gamma):\n",
    "            return np.sum([(gamma**k) * float(reward) for k, reward in enumerate(rewards)])\n",
    "        # Get probability weighted cumulated reward\n",
    "        cumulated_reward = np.sum([p * overall_reward(g['rewards'], self.gamma) \n",
    "                                   for p, g in zip(probabilities, self.groups)])\n",
    "        \n",
    "        return returned_rewards, cumulated_reward\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Further steps -> coding actor and critic network, replay memory, train functions and evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy function approximator --> Actor Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor():\n",
    "    \"\"\"Policy Function Approximator\"\"\"\n",
    "    def __init__(self,session,state_space_size,action_space_size,batch_size,embedding_size,\\\n",
    "                 tau,actor_learning_rate,action_len=1,history_len=10,scope='actor'):\n",
    "        self.session = session\n",
    "        self.state_space_size = state_space_size\n",
    "        self.action_space_size = action_space_size\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.tau = tau\n",
    "        self.actor_lr = actor_learning_rate\n",
    "        self.action_len = action_len\n",
    "        self.history_len = history_len\n",
    "        self.scope = scope\n",
    "        with tf.compat.v1.variable_scope(self.scope):\n",
    "            #Build estimator actor network\n",
    "            self.action_weights,self.state,self.sequence_length = self.build_net('estimate_actor')\n",
    "            self.network_params = tf.compat.v1.trainable_variables()\n",
    "            #Build target network\n",
    "            self.target_action_weights,self.target_state,self.target_sequence_len = self.build_net('target_actor')\n",
    "            #get network parameters for target_actor network\n",
    "            self.target_network_params =tf.compat.v1.trainable_variables()[len(self.network_params):]\n",
    "\n",
    "            # Initialize target network weights with network weights (θ^π′ ← θ^π)\n",
    "            self.init_target_network_params = [self.target_network_params[i].assign(self.network_params[i])\n",
    "            for i in range(len(self.target_network_params))]\n",
    "\n",
    "            # Update target network weights (θ^π′ ← τθ^π + (1 − τ)θ^π′)\n",
    "            self.update_target_network_params = [self.target_network_params[i].assign(\n",
    "                tf.multiply(self.tau, self.network_params[i]) +\n",
    "                tf.multiply(1 - self.tau, self.target_network_params[i])) \n",
    "                                                for i in range(len(self.target_network_params))]\n",
    "\n",
    "            self.action_gradient = tf.compat.v1.placeholder(tf.float32,[None,self.action_space_size])\n",
    "            gradients = tf.gradients(tf.reshape(self.action_weights,[self.batch_size,self.action_space_size]),\\\n",
    "                                     self.network_params,self.action_gradient)\n",
    "            self.params_gradient = list(map(\n",
    "                lambda x: tf.compat.v1.div(x,self.batch_size * self.action_space_size),gradients\n",
    "            ))\n",
    "            # Compute ∇_a.Q(s, a|θ^µ).∇_θ^π.f_θ^π(s)\n",
    "            self.optimizer = tf.compat.v1.train.AdamOptimizer(self.actor_lr).apply_gradients(\n",
    "                zip(self.params_gradient, self.network_params)\n",
    "            )\n",
    "            self.num_trainable_vars = len(self.network_params) + len(self.target_network_params)\n",
    "    \n",
    "\n",
    "    def build_net(self,scope):\n",
    "        \"\"\"Build Tensorflow Graph\"\"\"\n",
    "        def gather_last_output(data, seq_lens):\n",
    "            def cli_value(x, v):\n",
    "                y = tf.constant(v, shape=x.get_shape(), dtype=tf.int64)\n",
    "                x = tf.cast(x, tf.int64)\n",
    "                return tf.where(tf.greater(x, y), x, y)\n",
    "            batch_range = tf.range(tf.cast(tf.shape(data)[0], dtype=tf.int64), dtype=tf.int64)\n",
    "            tmp_end = tf.map_fn(lambda x: cli_value(x, 0), seq_lens - 1, dtype=tf.int64)\n",
    "            indices = tf.stack([batch_range, tmp_end], axis=1)\n",
    "            return tf.gather_nd(data, indices)  \n",
    "        \n",
    "        with tf.compat.v1.variable_scope(scope):\n",
    "            state = tf.compat.v1.placeholder(tf.float32,[None,self.state_space_size],\"state\")\n",
    "            state_ = tf.reshape(state,[-1,self.history_len,self.embedding_size])\n",
    "            sequence_length = tf.compat.v1.placeholder(tf.int32,[None],'sequence_length')\n",
    "            cell = tf.compat.v1.nn.rnn_cell.GRUCell(self.embedding_size,\n",
    "                                                   activation = tf.nn.relu,\n",
    "                                                   kernel_initializer = tf.initializers.random_normal(),\n",
    "                                                   bias_initializer = tf.zeros_initializer())\n",
    "            outputs,_ = tf.compat.v1.nn.dynamic_rnn(cell,state_,dtype=tf.float32,sequence_length=sequence_length)\n",
    "            last_output = gather_last_output(outputs,sequence_length)\n",
    "            x = tf.keras.layers.Dense(self.action_len * self.embedding_size)(last_output)\n",
    "            action_weights = tf.reshape(x,[-1,self.action_len,self.embedding_size])\n",
    "        return action_weights, state, sequence_length\n",
    "    def train(self,state,sequence_length,action_gradients):\n",
    "        \"\"\"\n",
    "      Compute ∇_a.Q(s, a|θ^µ).∇_θ^π.f_θ^π(s)\n",
    "        \"\"\"\n",
    "        self.session.run(self.optimizer,feed_dict={\n",
    "            self.state:state,\n",
    "            self.sequence_length:sequence_length,\n",
    "            self.action_gradient:action_gradients\n",
    "        })\n",
    "        \n",
    "    def predict(self,state,sequence_length):\n",
    "        return self.session.run(self.action_weights,feed_dict={\n",
    "            self.state:state,\n",
    "            self.sequence_length:sequence_length\n",
    "        })\n",
    "    def predict_target(self,state,sequence_length):\n",
    "        return self.session.run(self.target_action_weights,feed_dict={\n",
    "            self.target_state: state,\n",
    "            self.target_sequence_len:sequence_length\n",
    "        })\n",
    "    def init_target_network(self):\n",
    "        self.session.run(self.init_target_network_params)\n",
    "    def update_target_network(self):\n",
    "        self.session.run(self.update_target_network_params)\n",
    "    def get_num_trainable_vars(self):\n",
    "        return self.num_trainable_vars\n",
    "    \n",
    "    def get_recommendation(self,action_len,noisy_state,embeddings,target=False):\n",
    "        \"\"\"\n",
    "        Algorithm 2 from Listwise Recommendation Paper\n",
    "        Args:\n",
    "        action_len: length of recommendation list (K)\n",
    "        noisy_state: environment state with noise\n",
    "        embeddings: Embeddings class object\n",
    "        target: boolean to indicate use of Actor network or Target Network\n",
    "        \n",
    "        Returns:\n",
    "        Recommendation: list of embedded item as future actions\n",
    "        \"\"\"\n",
    "        def get_score(weights,embedding,batch_size):\n",
    "            return np.dot(weights,embedding.T)\n",
    "        \n",
    "        batch_size = noisy_state.shape[0]\n",
    "        method = self.predict_target if target else self.predict\n",
    "        weights = method(noisy_state,[action_len]*batch_size)\n",
    "        \n",
    "        scores = np.array([[[get_score(weights[i][j],embedding,batch_size)\n",
    "                            for embedding in embeddings.get_embedding_vector()]\n",
    "                           for j in range(action_len)]\n",
    "                          for i in range(batch_size)])\n",
    "        return np.array([[embeddings.get_embedding(np.argmax(scores[i][j]))\n",
    "                         for j in range(action_len)]\n",
    "                        for i in range(batch_size)])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value function approximator --> Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic():\n",
    "    \"\"\"Value Function Approximator\"\"\"\n",
    "    def __init__(self,session,state_space_size,action_space_size,embedding_size,\\\n",
    "                 tau,critic_learning_rate,history_len=10,scope='critic'):\n",
    "        self.session = session\n",
    "        self.state_space_size = state_space_size\n",
    "        self.action_space_size = action_space_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.tau = tau\n",
    "        self.learning_rate = critic_learning_rate\n",
    "        self.history_len = history_len\n",
    "        self.scope = scope\n",
    "        with tf.compat.v1.variable_scope(self.scope):\n",
    "            #Build critic Network\n",
    "            self.critic_Q_value,self.state,self.action,self.sequence_length = self.build_net('estimator_critic')\n",
    "            self.network_params = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES,\\\n",
    "                                                              scope='estimator_critic')\n",
    "            \n",
    "            #Build target Critic Network\n",
    "            self.target_Q_value,self.target_state,self.target_action,\\\n",
    "            self.target_sequence_length = self.build_net('target_critic')\n",
    "            self.target_network_params = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES,\\\n",
    "                                                           scope='target_critic')\n",
    "            \n",
    "            #Initialize target network weights with network weights\n",
    "            self.init_target_network_params = [self.target_network_params[i].assign(self.network_params[i]) \n",
    "                                               for i in range(len(self.target_network_params))]\n",
    "            #Update Target network weights\n",
    "            self.update_target_network_params = [self.target_network_params[i].assign(\n",
    "                tf.multiply(self.tau, self.network_params[i]) + \n",
    "                tf.multiply(1 - self.tau, self.target_network_params[i]))\n",
    "                for i in range(len(self.target_network_params))]\n",
    "            \n",
    "            #Minimize MSE between critic's Q values and target critic's output Q values\n",
    "            self.expected_reward = tf.compat.v1.placeholder(tf.float32,[None,1])\n",
    "            self.loss = tf.reduce_mean(tf.math.squared_difference(self.expected_reward,self.critic_Q_value))\n",
    "            self.optimizer = tf.compat.v1.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "            \n",
    "            #Compute action gradients  ∇_a.Q(s, a|θ^µ)\n",
    "            self.action_gradients = tf.gradients(self.critic_Q_value,self.action)\n",
    "    \n",
    "    def build_net(self,scope):\n",
    "        # Inputs: current state, current action\n",
    "        # Outputs: predicted Q-value\n",
    "        \n",
    "        def gather_last_output(data, seq_lens):\n",
    "            def cli_value(x, v):\n",
    "                y = tf.constant(v, shape=x.get_shape(), dtype=tf.int64)\n",
    "                return tf.where(tf.greater(x, y), x, y)\n",
    "            this_range = tf.range(tf.cast(tf.shape(seq_lens)[0], dtype=tf.int64), dtype=tf.int64)\n",
    "            tmp_end = tf.map_fn(lambda x: cli_value(x, 0), seq_lens - 1, dtype=tf.int64)\n",
    "            indices = tf.stack([this_range, tmp_end], axis=1)\n",
    "            return tf.gather_nd(data, indices)\n",
    "\n",
    "        with tf.compat.v1.variable_scope(scope):\n",
    "            state = tf.compat.v1.placeholder(tf.float32,[None,self.state_space_size],'state')\n",
    "            state_ = tf.reshape(state, [-1, self.history_len, self.embedding_size])\n",
    "            action = tf.compat.v1.placeholder(tf.float32, [None, self.action_space_size], 'action')\n",
    "            sequence_length = tf.compat.v1.placeholder(tf.int64, [None], name='critic_sequence_length')\n",
    "            cell = tf.compat.v1.nn.rnn_cell.GRUCell(self.history_len,\n",
    "                                        activation=tf.nn.relu,\n",
    "                                        kernel_initializer=tf.initializers.random_normal(),\n",
    "                                        bias_initializer=tf.zeros_initializer())\n",
    "            predicted_state, _ = tf.compat.v1.nn.dynamic_rnn(cell, state_, dtype=tf.float32, sequence_length=sequence_length)\n",
    "            predicted_state = gather_last_output(predicted_state, sequence_length)\n",
    "            \n",
    "            inputs = tf.concat([predicted_state, action], axis=-1)\n",
    "            layer1 = tf.keras.layers.Dense(32, activation=tf.nn.relu)(inputs)\n",
    "            layer2 = tf.keras.layers.Dense(16, activation=tf.nn.relu)(layer1)\n",
    "            critic_Q_value = tf.keras.layers.Dense(1)(layer2)\n",
    "        return critic_Q_value, state, action, sequence_length            \n",
    "        \n",
    "    def train(self,state,action,sequence_length,expected_reward):\n",
    "        \"\"\"\n",
    "        MINIMIZE MSE between target critic Q val and expected reward \n",
    "        \"\"\"\n",
    "        return self.session.run([self.critic_Q_value,self.loss,self.optimizer],\n",
    "                            feed_dict={\n",
    "                                self.state:state,\n",
    "                                self.action:action,\n",
    "                                self.sequence_length:sequence_length,\n",
    "                                self.expected_reward:expected_reward\n",
    "                            })\n",
    "    def predict(self,state,action,sequence_length):\n",
    "        \"\"\"\n",
    "        Return critic's predicted Q val\n",
    "        \"\"\"\n",
    "        return self.session.run(self.critic_Q_value,\n",
    "                               feed_dict={\n",
    "                                   self.state:state,\n",
    "                                   self.action:action,\n",
    "                                   self.sequence_length: sequence_length\n",
    "                               })\n",
    "    def predict_target(self, state, action, sequence_length):\n",
    "        \"\"\" \n",
    "        Returns target Critic's predicted Q-value. \n",
    "        \"\"\"\n",
    "        return self.session.run(self.target_Q_value,\n",
    "                             feed_dict={\n",
    "                                 self.target_state: state,\n",
    "                                 self.target_action: action,\n",
    "                                 self.target_sequence_length: sequence_length\n",
    "                             })\n",
    "    def get_action_gradients(self, state, action, sequence_length):\n",
    "        \"\"\"\n",
    "        Returns ∇_a.Q(s,a|θ^µ)\n",
    "        \"\"\"\n",
    "        return np.array(self.session.run(self.action_gradients,\n",
    "                             feed_dict={\n",
    "                                 self.state: state,\n",
    "                                 self.action: action,\n",
    "                                 self.sequence_length: sequence_length\n",
    "                             })[0])\n",
    "    \n",
    "    def init_target_network(self):\n",
    "        self.session.run(self.init_target_network_params)\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        self.session.run(self.update_target_network_params)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    \"\"\"\n",
    "    Replay Memory D in Listwise Recommendation Paper\n",
    "    \"\"\"\n",
    "    def __init__(self,buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = []\n",
    "    def add(self,state,action,reward,n_state):\n",
    "        self.buffer.append([state,action,reward,n_state])\n",
    "        if len(self.buffer) > self.buffer_size:\n",
    "            self.buffer.pop(0)\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "    def sample_batch(self,batch_size):\n",
    "        return random.sample(self.buffer,batch_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experience_replay(replay_memory,batch_size,actor,critic,\\\n",
    "                      embeddings,action_len,state_space_size,action_space_size,gamma):\n",
    "    \"\"\"\n",
    "    Experience Replay\n",
    "Args:\n",
    "    replay_memory: replay_memory class object\n",
    "    batch_size: sample_size\n",
    "    actor: Actor Network\n",
    "    critic: Critic Network\n",
    "    embeddings: Embeddings class object\n",
    "    state_space_size: dimension of states\n",
    "    action_space_size: dimension of actions\n",
    "Returns:\n",
    "    Best Q-value, loss of critic network\n",
    "    \"\"\"\n",
    "    samples = replay_memory.sample_batch(batch_size)\n",
    "    states = np.array([s[0] for s in samples])\n",
    "    actions = np.array([s[1] for s in samples])\n",
    "    rewards = np.array([s[2] for s in samples])\n",
    "    n_states = np.array([s[3] for s in samples]).reshape(-1, state_space_size)\n",
    "\n",
    "  # '23: Generate a′ by target Actor network according to Algorithm 2'\n",
    "    n_actions = actor.get_recommendation(action_len, states, embeddings, target=True).reshape(-1, action_space_size)\n",
    "\n",
    "  # Calculate predicted Q′(s′, a′|θ^µ′) value\n",
    "    target_Q_value = critic.predict_target(n_states, n_actions, [action_len] * batch_size)\n",
    "\n",
    "  # '24: Set y = r + γQ′(s′, a′|θ^µ′)'\n",
    "    expected_rewards = rewards + gamma * target_Q_value\n",
    "  \n",
    "  # '25: Update Critic by minimizing (y − Q(s, a|θ^µ))²'\n",
    "    critic_Q_value, critic_loss, _ = critic.train(states, actions, [action_len] * batch_size, expected_rewards)\n",
    "  \n",
    "  # '26: Update the Actor using the sampled policy gradient'\n",
    "    action_gradients = critic.get_action_gradients(states, n_actions, [action_len] * batch_size)\n",
    "    actor.train(states, [action_len] * batch_size, action_gradients)\n",
    "\n",
    "  # '27: Update the Critic target networks'\n",
    "    critic.update_target_network()\n",
    "\n",
    "  # '28: Update the Actor target network'\n",
    "    actor.update_target_network()\n",
    "\n",
    "    return np.amax(critic_Q_value), critic_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrnsteinUhlenbeckNoise:\n",
    "    \"\"\"Noise for Actor Predictions\"\"\"\n",
    "    def __init__(self,action_space_size,mu=0,theta=0.5,sigma=0.2):\n",
    "        self.action_space_size = action_space_size\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = np.ones(self.action_space_size) * self.mu\n",
    "    def get(self):\n",
    "        self.state += self.theta * (self.mu - self.state) + self.sigma*np.random.rand(self.action_space_size)\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(session,env,actor,critic,embeddings,history_len,action_len,buffer_size,batch_size,\\\n",
    "         gamma,nb_episodes,nb_rounds,filename_summary):\n",
    "    \"\"\"Algorithm 3 in Listwise Recommendation Paper\"\"\"\n",
    "    def build_summary():\n",
    "        episode_reward = tf.Variable(0.)\n",
    "        tf.summary.scalar('reward',episode_reward)\n",
    "        episode_max_Q = tf.Variable(0.)\n",
    "        tf.summary.scalar('max_Q_value',episode_max_Q)\n",
    "        critic_loss = tf.Variable(0.)\n",
    "        tf.summary.scalar('critic_loss',critic_loss)\n",
    "        \n",
    "        summary_vars = [episode_reward,episode_max_Q,critic_loss]\n",
    "        summary_ops = tf.compat.v1.summary.merge_all()\n",
    "        return summary_ops,summary_vars\n",
    "    \n",
    "    summary_ops,summary_vars = build_summary()\n",
    "    session.run(tf.compat.v1.global_variables_initializer())    \n",
    "    writer = tf.compat.v1.summary.FileWriter(filename_summary,session.graph)\n",
    "\n",
    "    actor.init_target_network()\n",
    "    critic.init_target_network()\n",
    "    \n",
    "    replay_memory = ReplayMemory(buffer_size)\n",
    "    replay = False\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for i_session in range(nb_episodes):\n",
    "        session_reward = 0\n",
    "        session_Q_value = 0\n",
    "        session_critic_loss = 0\n",
    "        \n",
    "        states = env.reset()#Initialize state s0 from previous sessions\n",
    "        if (i_session + 1) % 10 == 0: #Update average parameters every 10 episodes\n",
    "            env.groups = env.get_groups()\n",
    "        \n",
    "#         exploration_noise = OrnsteinUhlenbeckNoise(history_len * embeddings.size())\n",
    "        for t in range(nb_rounds):\n",
    "            #select actions according to get recommendation list\n",
    "            exploration_noise = OrnsteinUhlenbeckNoise(len(states) * embeddings.size())\n",
    "            actions = actor.get_recommendation(action_len,\\\n",
    "                                               states.reshape(1,-1) + exploration_noise.get().reshape(1,-1),\n",
    "                                               embeddings\n",
    "                                              ).reshape(action_len,embeddings.size())\n",
    "            rewards,next_states,done,_ = env.step(actions)\n",
    "            \n",
    "            replay_memory.add(states.reshape(history_len * embeddings.size()),\n",
    "                             actions.reshape(action_len * embeddings.size()),\n",
    "                             [rewards],\n",
    "                             next_states.reshape(len(next_states)*embeddings.size()))\n",
    "            states = next_states\n",
    "            session_reward += rewards\n",
    "            \n",
    "            #parameter update\n",
    "            if replay_memory.size() >= batch_size:\n",
    "                replay = True\n",
    "                replay_Q_value, critic_loss = experience_replay(replay_memory,batch_size,\n",
    "                                                               actor,critic,embeddings,action_len,\\\n",
    "                                                               history_len * embeddings.size(),\n",
    "                                                               action_len * embeddings.size(),\n",
    "                                                               gamma)\n",
    "                session_Q_value += replay_Q_value\n",
    "                session_critic_loss += critic_loss\n",
    "                print(\"Session Reward: {}, Session_Q_value: {}, Session_critic_loss: {}\".format(session_reward,\n",
    "                                                                                               session_Q_value,\n",
    "                                                                                               session_critic_loss))\n",
    "#             summary_str = session.run(summary_ops,\n",
    "#                                         feed_dict = {\n",
    "#                                          summary_vars[0]: session_reward,\n",
    "#                                          summary_vars[1]: session_Q_value,\n",
    "#                                          summary_vars[2]: session_critic_loss\n",
    "#                                      })\n",
    "#             writer.add_summary(summary_str,i_session)\n",
    "        str_loss = str('Loss=%0.4f' % session_critic_loss)\n",
    "        print(('Episode %d/%d Reward=%d Time=%ds ' + (str_loss if replay else 'No replay')) % (i_session + 1, nb_episodes, session_reward, time.time() - start_time))\n",
    "        start_time = time.time()\n",
    "    writer.close()\n",
    "    tf.compat.v1.train.Saver().save(session,'models.h5',write_meta_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparams\n",
    "history_len = 10\n",
    "action_len = 1\n",
    "gamma = 0.99\n",
    "actor_lr = 0.0001\n",
    "critic_lr = 0.001\n",
    "tau = 0.001\n",
    "batch_size = 64\n",
    "nb_episodes = 100\n",
    "nb_rounds = 50\n",
    "filename_summary = 'summary.txt'\n",
    "alpha = 0.5\n",
    "gamma = 0.9\n",
    "buffer_size = 1000000\n",
    "fixed_len = True\n",
    "state_space_size = embeddings.size() * history_len\n",
    "action_space_size = embeddings.size() * action_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test code\n",
    "item_map = create_item_mappings(embeddings_df)\n",
    "embeddings = Embeddings(read_embeddings(embeddings_df),item_map)\n",
    "data = file_convert(sequence_df.copy())\n",
    "register_env()\n",
    "env = gym.make('recsys-v0',data=data,embeddings=embeddings,alpha=alpha,gamma=gamma,fixed_length=fixed_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 23:09:38.996976: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2021-12-02 23:09:38.997952: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "session = tf.compat.v1.Session()\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "# Initialize actor network f_θ^π and critic network Q(s, a|θ^µ) with random weights\n",
    "actor = Actor(session, state_space_size, action_space_size, batch_size, embeddings.size(),tau,actor_lr,\\\n",
    "              action_len, history_len)\n",
    "critic = Critic(session, state_space_size, action_space_size, embeddings.size(),\\\n",
    "                tau, critic_lr,history_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 23:09:39.802006: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-12-02 23:09:40.114496: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-12-02 23:09:40.137782: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/5 Reward=150 Time=1s No replay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 23:09:42.384992: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-12-02 23:09:44.275603: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-12-02 23:09:44.583415: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-12-02 23:09:45.227751: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-12-02 23:09:45.652299: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-12-02 23:09:46.082693: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session Reward: 42.092785101640004, Session_Q_value: 0.459926575422287, Session_critic_loss: 14.723581314086914\n",
      "Session Reward: 45.10483022236753, Session_Q_value: 0.9953482449054718, Session_critic_loss: 29.06245994567871\n",
      "Session Reward: 48.10324803836691, Session_Q_value: 1.6132010519504547, Session_critic_loss: 43.02553653717041\n",
      "Session Reward: 51.103698883765766, Session_Q_value: 2.3141282498836517, Session_critic_loss: 56.62072944641113\n",
      "Session Reward: 54.101801918194965, Session_Q_value: 3.099524587392807, Session_critic_loss: 69.9608907699585\n",
      "Session Reward: 57.10094370322532, Session_Q_value: 3.9694182574748993, Session_critic_loss: 82.97946739196777\n",
      "Session Reward: 60.11096215338735, Session_Q_value: 4.924492031335831, Session_critic_loss: 95.4597225189209\n",
      "Session Reward: 63.116013637630466, Session_Q_value: 5.964443117380142, Session_critic_loss: 107.71192932128906\n",
      "Session Reward: 66.11555652848918, Session_Q_value: 7.089667707681656, Session_critic_loss: 119.57706260681152\n",
      "Session Reward: 69.11679053018324, Session_Q_value: 8.299100071191788, Session_critic_loss: 131.13656616210938\n",
      "Session Reward: 72.12688791058004, Session_Q_value: 9.592675477266312, Session_critic_loss: 142.5372486114502\n",
      "Session Reward: 75.12704413266061, Session_Q_value: 10.968231350183487, Session_critic_loss: 153.66050624847412\n",
      "Session Reward: 78.12615872750114, Session_Q_value: 12.426821500062943, Session_critic_loss: 164.55766677856445\n",
      "Session Reward: 81.12487503990232, Session_Q_value: 13.968173295259476, Session_critic_loss: 175.01416969299316\n",
      "Session Reward: 84.12306625225432, Session_Q_value: 15.59122171998024, Session_critic_loss: 185.26745891571045\n",
      "Session Reward: 87.12255986659652, Session_Q_value: 17.292983442544937, Session_critic_loss: 195.3604507446289\n",
      "Session Reward: 90.1236187118506, Session_Q_value: 19.07317963242531, Session_critic_loss: 205.09184646606445\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sd/gpl4cvx11js9jwvpk7zwlsp00000gn/T/ipykernel_1448/2411284517.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train(session, env, actor, critic, embeddings, history_len, action_len,\\\n\u001b[0m\u001b[1;32m      2\u001b[0m       buffer_size,batch_size,gamma,nb_episodes,nb_rounds,filename_summary=filename_summary)\n",
      "\u001b[0;32m/var/folders/sd/gpl4cvx11js9jwvpk7zwlsp00000gn/T/ipykernel_1448/1835055410.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(session, env, actor, critic, embeddings, history_len, action_len, buffer_size, batch_size, gamma, nb_episodes, nb_rounds, filename_summary)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreplay_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mreplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 replay_Q_value, critic_loss = experience_replay(replay_memory,batch_size,\n\u001b[0m\u001b[1;32m     57\u001b[0m                                                                \u001b[0mactor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                                                                \u001b[0mhistory_len\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/sd/gpl4cvx11js9jwvpk7zwlsp00000gn/T/ipykernel_1448/2520875580.py\u001b[0m in \u001b[0;36mexperience_replay\u001b[0;34m(replay_memory, batch_size, actor, critic, embeddings, action_len, state_space_size, action_space_size, gamma)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0;31m# '25: Update Critic by minimizing (y − Q(s, a|θ^µ))²'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mcritic_Q_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0maction_len\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0;31m# '26: Update the Actor using the sampled policy gradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/sd/gpl4cvx11js9jwvpk7zwlsp00000gn/T/ipykernel_1448/1818617057.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, state, action, sequence_length, expected_reward)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mMINIMIZE\u001b[0m \u001b[0mMSE\u001b[0m \u001b[0mbetween\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0mcritic\u001b[0m \u001b[0mQ\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \"\"\"\n\u001b[0;32m---> 77\u001b[0;31m         return self.session.run([self.critic_Q_value,self.loss,self.optimizer],\n\u001b[0m\u001b[1;32m     78\u001b[0m                             feed_dict={\n\u001b[1;32m     79\u001b[0m                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RL_project/.venv/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[1;32m    968\u001b[0m                          run_metadata_ptr)\n\u001b[1;32m    969\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RL_project/.venv/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[0;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[1;32m   1191\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[1;32m   1192\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RL_project/.venv/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1368\u001b[0;31m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[1;32m   1369\u001b[0m                            run_metadata)\n\u001b[1;32m   1370\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RL_project/.venv/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1373\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RL_project/.venv/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0m\u001b[1;32m   1360\u001b[0m                                       target_list, run_metadata)\n\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RL_project/.venv/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1449\u001b[0m   def _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list,\n\u001b[1;32m   1450\u001b[0m                           run_metadata):\n\u001b[0;32m-> 1451\u001b[0;31m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0m\u001b[1;32m   1452\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m                                             run_metadata)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(session, env, actor, critic, embeddings, history_len, action_len,\\\n",
    "      buffer_size,batch_size,gamma,nb_episodes,nb_rounds,filename_summary=filename_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
